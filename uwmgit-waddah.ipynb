{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## libraries for the unet model\n!pip install -q -U segmentation-models-pytorch albumentations > /dev/null","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:47:54.491191Z","iopub.execute_input":"2022-05-22T11:47:54.491550Z","iopub.status.idle":"2022-05-22T11:48:09.498544Z","shell.execute_reply.started":"2022-05-22T11:47:54.491512Z","shell.execute_reply":"2022-05-22T11:48:09.497717Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\nimport os\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor, Resize\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport torchvision.transforms as T\nimport albumentations as A\nimport cv2\nfrom torchvision.transforms import ToPILImage as to_PIL\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:08.562200Z","iopub.execute_input":"2022-05-22T11:51:08.562989Z","iopub.status.idle":"2022-05-22T11:51:08.570501Z","shell.execute_reply.started":"2022-05-22T11:51:08.562946Z","shell.execute_reply":"2022-05-22T11:51:08.569628Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# CONFIGURATIONS (GLOBAL VARIABLES)","metadata":{}},{"cell_type":"code","source":"class config:\n    SEEDS = 42\n    BATCH_SIZE = 8\n    IM_SIZE = (256, 256)\n    NUM_CLASSES = 3\n    epochs = 10\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:12.112987Z","iopub.execute_input":"2022-05-22T11:51:12.113575Z","iopub.status.idle":"2022-05-22T11:51:12.118073Z","shell.execute_reply.started":"2022-05-22T11:51:12.113534Z","shell.execute_reply":"2022-05-22T11:51:12.117393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    Dieu nay de tai lap'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(config.SEEDS)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:14.954185Z","iopub.execute_input":"2022-05-22T11:51:14.954730Z","iopub.status.idle":"2022-05-22T11:51:14.965432Z","shell.execute_reply.started":"2022-05-22T11:51:14.954687Z","shell.execute_reply":"2022-05-22T11:51:14.964737Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# META DATA PREPARATION :(Necessary Functions)","metadata":{}},{"cell_type":"code","source":"##_______________________________________________META DATA PREPROCESSING FUNCTIONS_______________________________________________________________\ndef get_case_day_silce_from_id(df):\n    df['case_id'] = df['id'].apply(lambda x: x.split('_')[0][4:])\n    df['day_id'] = df['id'].apply(lambda x: x.split('_')[1][3:])\n    df['slice_id'] = df['id'].apply(lambda x: x.split('_')[-1])\n    return df\ndef fetch_file_from_id(root_dir, case_id):\n    case_folder = case_id.split('_')[0]\n    day_folder = '_'.join(case_id.split('_')[:2])\n    file_starter = '_'.join(case_id.split('_')[2:])\n    folder = os.path.join(root_dir, case_folder, day_folder, 'scans')\n    file = glob(f'{folder}/{file_starter}*')\n    return file[0]\ndef preprocess_metadata(root_dir, df):\n    df['segmentation'] = df['segmentation'].astype('str') \n    df = get_case_day_silce_from_id(df)\n    df['path'] = df['id'].apply(lambda x: fetch_file_from_id(root_dir, x))\n    df['height'] = df['path'].apply(lambda x: os.path.split(x)[-1].split('_')[2]).astype('int')\n    df['width'] = df['path'].apply(lambda x: os.path.split(x)[-1].split('_')[3]).astype('int')\n    class_names = df['class'].unique()\n    for ind, label in enumerate(class_names):\n        df['class'].replace(label, ind, inplace=True)\n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:23.506868Z","iopub.execute_input":"2022-05-22T11:51:23.507137Z","iopub.status.idle":"2022-05-22T11:51:23.518762Z","shell.execute_reply.started":"2022-05-22T11:51:23.507096Z","shell.execute_reply":"2022-05-22T11:51:23.517966Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# DATASET PREPARATION: (MASK GENERATION & IMAGE)\n* process the image data shapes form the image ids.\n* Process mask from RLE format.","metadata":{}},{"cell_type":"code","source":"##---------------------------------------------------Preprare DATASET CLASS ------------------------------------------------------------------\n\n#Inherited from pytorch's Datatset class. (torch.utils.data)\nclass UWDataset(Dataset):\n    def __init__(self, meta_df, H = 256, W = 256, preprocessing = None):\n        super().__init__()\n        self.meta_df = meta_df\n        self.h = H\n        self.w = W\n        self.resize = A.Resize(H,W)\n        self.preprocessing = preprocessing #recommended to preprocess the data as the data format the unet model was pretrained to get more accurate results\n    def __len__(self):\n        return len(self.meta_df)\n    def __getitem__(self, index):\n        path = self.meta_df.loc[index, 'path']\n        image = self.load_image(path) # ndarray loaded using cv2\n        mask_h, mask_w = self.meta_df.loc[index, 'height'], self.meta_df.loc[index, 'width']\n        mask_string = self.meta_df.loc[index, 'segmentation']\n        mask_channel = self.load_mask(string = mask_string, h= mask_h, w = mask_w)\n#         if self.preprocessing:\n#             image, mask_channel = np.array(image),np.array(mask_channel)\n#             sample = self.preprocessing(image= image, mask = mask_channel)\n#             image, mask_channel = Image.fromarray(sample['image']),Image.fromarray(sample['mask'])\n        #image = ToTensor()(self.resize(image))\n        image = image.transpose(2,0,1) # (C,H, W)\n        image = torch.from_numpy(image)\n        #mask_channel = ToTensor()(self.resize(mask_channel))\n        mask_channel = torch.from_numpy(mask_channel)\n        mask = torch.zeros((3, self.h, self.w))\n        class_label = self.meta_df.loc[index, 'class']\n        mask[class_label, ...] = mask_channel\n        #mask = mask_channel\n        if self.preprocessing:\n            image, mask = np.array(image),np.array(mask)\n            sample = self.preprocessing(image= image, mask = mask)\n            image, mask = sample['image'],sample['mask']\n            #print(f'Shapes: {image.shape} , {mask.shape}')\n            \n#             mask = (mask / 255).astype(np.float32)\n#             mask = np.expand_dims(mask, axis=0)\n#             image = (image / 255).astype(np.float32)\n#             image = np.expand_dims(image, axis=0)\n        return torch.tensor(image), torch.tensor(mask) #TODO: Add transpose for the returned mask and image\n    def prepare_mask_data(self,string):\n        all_values = map(int, string.split(' '))\n        start_indx, pixel_counts = [], []\n        for i, value in enumerate(all_values):\n            if i%2==0:\n                start_indx.append(value)\n            else:\n                pixel_counts.append(value)\n        return start_indx, pixel_counts\n    def fetch_pos_pixel_indexes(self, indexes, counts):\n        final_Arr = []\n        for ind,count in zip(indexes, counts):\n            final_Arr += [ind + i for i in range(count)]\n        return final_Arr\n    def prepare_mask(self, string, h,w):\n        indexes, counts = self.prepare_mask_data(string=string)\n        pos_pixel_indexes = self.fetch_pos_pixel_indexes(indexes, counts)\n        mask_arr = np.zeros(h*w)\n        mask_arr[pos_pixel_indexes] = 1\n        return mask_arr.reshape(h,w)\n    def load_mask(self, string, h, w):\n        if string != 'nan':\n            #return Image.fromarray(self.prepare_mask(string=string, h=h, w=w))\n            return  cv2.resize(self.prepare_mask(string, h, w), config.IM_SIZE)\n        #return Image.fromarray(np.zeros((h,w)))\n        return cv2.resize(np.zeros((h,w)), config.IM_SIZE)\n    \n#     def load_image(self, path):\n#         return Image.open(path).convert('RGB')\n    def load_image(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        img = (img - img.min())/(img.max() - img.min())*255.0 \n        img = cv2.resize(img, config.IM_SIZE)\n        img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n        img = img.astype(np.float32) /255.\n        return img\n        #return np.zeros((h,w))\n    \ndef to_tensor(self, x, **kwargs):\n        #return ToTensor()(Resize(x))\n        return x.transpose(2,0,1).astype('float32')\n    \ndef get_preprocessing(preprocessing_fn):\n    transform = [\n    A.Lambda(image = preprocessing_fn),\n    A.Lambda(image = to_tensor, mask = to_tensor)\n    ]\n    return album.Compose(transform)\n#------------------------------------------------------------SHOW IMAGE----------------------------------------------------\ndef show_image(image_tensor, name):\n    plt.figure(figsize= (20,20))\n    plt.imshow(image_tensor.permute(1,2,0))\n    plt.title(name, size =30)\n    plt.show()\ndef show_batch(image_batch, mask_batch, name):\n    image_mask_batch = torch.cat([make_grid(image_batch, nrows = 8), make_grid(mask_batch, nrows = 8)], dim = 2)\n    show_image(image_mask_batch, name)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:54:53.637399Z","iopub.execute_input":"2022-05-21T11:54:53.637673Z","iopub.status.idle":"2022-05-21T11:54:53.660703Z","shell.execute_reply.started":"2022-05-21T11:54:53.637642Z","shell.execute_reply":"2022-05-21T11:54:53.660014Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN TEST SPLIT FUNCTION FOR TRAINING PROCESS\n1. Train_Test_split the metadata\n2. Load the train_validation(test) data batches that will be fed to train the model using DataLoader","metadata":{}},{"cell_type":"code","source":"def train_test_split(ds, test_size=0.2):\n    test_size = int(test_size*len(ds))\n    train_size = len(ds) - test_size\n    train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, test_size], generator = torch.Generator().manual_seed(config.SEEDS))\n    print(f'Length of training split: {len(train_ds)}')\n    print(f'Length of validation split: {len(val_ds)}')\n    train_dl = DataLoader(train_ds, batch_size = config.BATCH_SIZE, shuffle = True, drop_last = True)\n    val_dl = DataLoader(val_ds, batch_size = config.BATCH_SIZE, shuffle = True, drop_last = True)\n    return train_dl, val_dl","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:36:07.514280Z","iopub.execute_input":"2022-05-21T11:36:07.514548Z","iopub.status.idle":"2022-05-21T11:36:07.520716Z","shell.execute_reply.started":"2022-05-21T11:36:07.514517Z","shell.execute_reply":"2022-05-21T11:36:07.520033Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 1. Define training data and metadata directories","metadata":{}},{"cell_type":"code","source":"## Define the training and metadata pathes\ntrain_dir = '../input/uw-madison-gi-tract-image-segmentation/train'\ntrain_metadata_dir = '../input/uw-madison-gi-tract-image-segmentation/train.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:30:43.620701Z","iopub.execute_input":"2022-05-21T11:30:43.621443Z","iopub.status.idle":"2022-05-21T11:30:43.625249Z","shell.execute_reply.started":"2022-05-21T11:30:43.621405Z","shell.execute_reply":"2022-05-21T11:30:43.624173Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 2. Read training metadata data frame","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(train_metadata_dir)\ntrain_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:30:45.136467Z","iopub.execute_input":"2022-05-21T11:30:45.137020Z","iopub.status.idle":"2022-05-21T11:30:45.693465Z","shell.execute_reply.started":"2022-05-21T11:30:45.136981Z","shell.execute_reply":"2022-05-21T11:30:45.692805Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 3. Preprocessing metadata data frame: \n1. Convert NANs to string \n2. Get case, day and slice ids from id column\n3. fetch files from id and create a path column\n4. get the shape of each slice from path and implement them in the metadata\n5. perform label encoding to the class names in class columns","metadata":{}},{"cell_type":"code","source":"train_df = preprocess_metadata(train_dir, train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:30:45.977330Z","iopub.execute_input":"2022-05-21T11:30:45.979646Z","iopub.status.idle":"2022-05-21T11:32:11.424346Z","shell.execute_reply.started":"2022-05-21T11:30:45.979602Z","shell.execute_reply":"2022-05-21T11:32:11.423632Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 4. Load the training data using training metadata dataframe","metadata":{}},{"cell_type":"code","source":"ds = UWDataset(train_df)\nprint(f\"Length of the dataset : {len(ds)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:55:00.583617Z","iopub.execute_input":"2022-05-21T11:55:00.583884Z","iopub.status.idle":"2022-05-21T11:55:00.589298Z","shell.execute_reply.started":"2022-05-21T11:55:00.583853Z","shell.execute_reply":"2022-05-21T11:55:00.588566Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"image, mask = ds[194]\nimage.shape, mask.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:55:03.868994Z","iopub.execute_input":"2022-05-21T11:55:03.869385Z","iopub.status.idle":"2022-05-21T11:55:03.899040Z","shell.execute_reply.started":"2022-05-21T11:55:03.869351Z","shell.execute_reply":"2022-05-21T11:55:03.898322Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"combined_im_mask = torch.cat((image, mask), dim=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:32:11.797888Z","iopub.status.idle":"2022-05-21T11:32:11.798427Z","shell.execute_reply.started":"2022-05-21T11:32:11.798189Z","shell.execute_reply":"2022-05-21T11:32:11.798215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(combined_im_mask, 'Image & Mask')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T11:32:11.799478Z","iopub.status.idle":"2022-05-21T11:32:11.800028Z","shell.execute_reply.started":"2022-05-21T11:32:11.799798Z","shell.execute_reply":"2022-05-21T11:32:11.799824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. split loaded data to training and validation splits:\n","metadata":{}},{"cell_type":"code","source":"## split data to training and validation splits:\ntrain_dl, val_dl = train_test_split(ds, test_size =0.2)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-21T11:32:11.801089Z","iopub.status.idle":"2022-05-21T11:32:11.801637Z","shell.execute_reply.started":"2022-05-21T11:32:11.801395Z","shell.execute_reply":"2022-05-21T11:32:11.801421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. visualization of one batch of training data and validation data","metadata":{}},{"cell_type":"code","source":"for train_image_dl, train_mask_dl in train_dl:\n    break\nfor val_image_dl, val_mask_dl in val_dl:\n    break\nshow_batch(train_image_dl,train_mask_dl, 'Training batch')\nshow_batch(val_image_dl, val_mask_dl, 'Validation batch')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-21T11:32:11.802690Z","iopub.status.idle":"2022-05-21T11:32:11.803241Z","shell.execute_reply.started":"2022-05-21T11:32:11.803009Z","shell.execute_reply":"2022-05-21T11:32:11.803035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. load the Unet model to be trained on the loaded data","metadata":{}},{"cell_type":"code","source":"ENCODER = 'resnet18'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = train_df['class'].unique()\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n\ndata_transforms = A.Compose([\n        #A.Resize(*config.IM_SIZE, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.OneOf([\n                A.RandomContrast(),\n                A.RandomGamma(),\n                A.RandomBrightness(),\n                ], p=0.2),\n\n        ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:05:04.309702Z","iopub.execute_input":"2022-05-21T12:05:04.310421Z","iopub.status.idle":"2022-05-21T12:05:04.656107Z","shell.execute_reply.started":"2022-05-21T12:05:04.310382Z","shell.execute_reply":"2022-05-21T12:05:04.655370Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### preprocessing and loading data for the unet model","metadata":{}},{"cell_type":"code","source":"\nds_unet = UWDataset(train_df, preprocessing = data_transforms)\nprint(f\"Length of the dataset : {len(ds_unet)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:05:10.389168Z","iopub.execute_input":"2022-05-21T12:05:10.389418Z","iopub.status.idle":"2022-05-21T12:05:10.396444Z","shell.execute_reply.started":"2022-05-21T12:05:10.389390Z","shell.execute_reply":"2022-05-21T12:05:10.395698Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"im , msk = ds_unet[2]\nim.shape, msk.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:05:11.916472Z","iopub.execute_input":"2022-05-21T12:05:11.917071Z","iopub.status.idle":"2022-05-21T12:05:11.938913Z","shell.execute_reply.started":"2022-05-21T12:05:11.917034Z","shell.execute_reply":"2022-05-21T12:05:11.938283Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"## split data to training and validation splits:\ntrain_unet_dataset, test_unet_dataset = train_test_split(ds_unet, test_size =0.2)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:07:53.252311Z","iopub.execute_input":"2022-05-21T12:07:53.252569Z","iopub.status.idle":"2022-05-21T12:07:53.278486Z","shell.execute_reply.started":"2022-05-21T12:07:53.252539Z","shell.execute_reply":"2022-05-21T12:07:53.277672Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\nTRAINING = True\n\n# Set num of epochs\nEPOCHS = config.epochs\n\n# Set device: `cuda` or `cpu`\nDEVICE = config.device\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])\n\n# define learning rate scheduler (not used in this NB)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n)\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('/home/human/WADDAH/KAGGLE/unet-with-pretrained-resnet50-encoder-pytorch/best_model.pth'):\n    model = torch.load('/home/human/WADDAH/KAGGLE/unet-with-pretrained-resnet50-encoder-pytorch/best_model.pth', map_location=DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:07:24.583219Z","iopub.execute_input":"2022-05-21T12:07:24.583677Z","iopub.status.idle":"2022-05-21T12:07:24.593243Z","shell.execute_reply.started":"2022-05-21T12:07:24.583641Z","shell.execute_reply":"2022-05-21T12:07:24.591023Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T12:07:28.154155Z","iopub.execute_input":"2022-05-21T12:07:28.154673Z","iopub.status.idle":"2022-05-21T12:07:33.088949Z","shell.execute_reply.started":"2022-05-21T12:07:28.154636Z","shell.execute_reply":"2022-05-21T12:07:33.088191Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html -U","metadata":{"execution":{"iopub.status.busy":"2022-05-20T12:06:23.783099Z","iopub.status.idle":"2022-05-20T12:06:23.783924Z","shell.execute_reply.started":"2022-05-20T12:06:23.783602Z","shell.execute_reply":"2022-05-20T12:06:23.783637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n        \n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_unet_dataset)\n        valid_logs = valid_epoch.run(test_unet_dataset)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')\n\nprint(torch.__version__)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-21T12:07:57.442685Z","iopub.execute_input":"2022-05-21T12:07:57.443393Z","iopub.status.idle":"2022-05-21T18:10:33.201468Z","shell.execute_reply.started":"2022-05-21T12:07:57.443356Z","shell.execute_reply":"2022-05-21T18:10:33.200746Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-20T13:36:22.964544Z","iopub.execute_input":"2022-05-20T13:36:22.965010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\nimport sys\nprint('A', sys.version)\nprint('B', torch.__version__)\nprint('C', torch.cuda.is_available())\nprint('D', torch.backends.cudnn.enabled)\ndevice = torch.device('cuda')\nprint('E', torch.cuda.get_device_properties(device))\nprint('F', torch.tensor([1.0, 2.0]).cuda())","metadata":{"execution":{"iopub.status.busy":"2022-05-20T12:06:23.789985Z","iopub.status.idle":"2022-05-20T12:06:23.790817Z","shell.execute_reply.started":"2022-05-20T12:06:23.790464Z","shell.execute_reply":"2022-05-20T12:06:23.790497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}